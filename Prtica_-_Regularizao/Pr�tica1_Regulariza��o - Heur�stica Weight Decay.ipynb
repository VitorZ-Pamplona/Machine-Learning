{"cells":[{"cell_type":"markdown","metadata":{"id":"sKPfuEnapDnU"},"source":["# \u003cfont color=\"darkblue\"\u003e Prática 01: Regularização - Heurística Weight Decay \u003c/font\u003e"]},{"cell_type":"markdown","metadata":{"id":"nKa5whanpntS"},"source":["**Objetivos:**\n","\n","\n","*   Testar a Regressão logística com outra base de dados\n","*   Implementar a heurística *weight decay* para regularização da função inferida pelo algoritmo de aprendizagem \n","\n","**Requisitos de execução:**\n","\n","\n","*   Upload dos arquivos *logisticregression.py* e *diabetes.csv*"]},{"cell_type":"markdown","metadata":{"id":"OPCbV-Udr1Pz"},"source":["**Atividade 1:**\n","\n","1. Visitar a base de dados: https://www.kaggle.com/uciml/pima-indians-diabetes-database\n","2. Carregar os dados do arquivo *diabetes.csv* utilizando o pandas.\n","\n","    "]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"background_save":true},"id":"ZROjmwlFocyd"},"outputs":[],"source":["import pandas as pd \n","\n","diabete = pd.read_csv(\"diabetes.csv\", sep=',')\n","\n","# Pega o cabecalho do arquivo\n","print(diabete.head())"]},{"cell_type":"markdown","metadata":{"id":"O73AMlZRtOTU"},"source":["**Atividade 2:**\n","\n","1. Extrair os valores do *DataFrame* pandas e colocar nas variáveis\n"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"background_save":true},"id":"sU9ugDeOtaHd"},"outputs":[],"source":["Features = ['Pregnancies', 'Glucose', 'BloodPressure', 'SkinThickness',  'Insulin', 'BMI', 'DiabetesPedigreeFunction', 'Age']\n","X = diabete[Features].values\n","y = diabete.Outcome.values\n","\n","print(X)\n","print(y)"]},{"cell_type":"markdown","metadata":{"id":"LWlIeFootavy"},"source":["**Atividade 3:**\n","\n","1. Separar os dados em conjunto de treinamento e teste\n"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"background_save":true},"id":"1CEvKox1tsWj"},"outputs":[],"source":["from sklearn.model_selection import train_test_split\n","from sklearn.preprocessing import StandardScaler\n","\n","x_T, x_test, y_T, y_test = train_test_split(X, y, test_size=0.2, random_state=2698)\n","\n","sc = StandardScaler()\n","x_T = sc.fit_transform(x_T)\n","x_test = sc.fit_transform(x_test)\n","\n","print(\"Tamanho treino: \" + str(len(x_T)))\n","print(\"Tamanho teste: \" + str(len(x_test)))"]},{"cell_type":"markdown","metadata":{"id":"gGjfXNZ-tsor"},"source":["**Atividade 4:**\n","\n","1. Utilize a classe LogisticRegression, importada do pacote *sklearn.metrics*, para inferir aprendizado dos dados de treinamento;\n","2. Compute as métricas de aprendizado sobre os dados de teste."]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"background_save":true},"id":"jvw7C1Odt3Jb"},"outputs":[],"source":["from sklearn.linear_model import LogisticRegression\n","from sklearn.metrics import classification_report\n","\n","model = LogisticRegression(solver='newton-cg')\n","model.fit(x_T, y_T)\n","\n","print(classification_report(y_test, model.predict(x_test)))"]},{"cell_type":"markdown","metadata":{"id":"0SH_p2JeTmEN"},"source":["**Atividade 5:**\n","\n","1. Utilize a classe LogisticRegression, importada do arquivo *logisticregression.py*, para inferir aprendizado dos dados de treinamento;\n","2. Compute o $E_{in}$ sobre os dados de treino e o $E_{out}$ sobre os dados de teste;\n","2. Compute as métricas de aprendizado sobre os dados de teste."]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"background_save":true},"id":"GbnZN9PUQQ0d"},"outputs":[],"source":["from sklearn.metrics import classification_report\n","from logisticregression import LogisticRegression\n","\n","lrY = [+1 if value == 1 else -1 for value in y_T]\n","lr_y_test = [+1 if value == 1 else -1 for value in y_test]\n","\n","#Regressão Logistica\n","classifier = LogisticRegression(0.1, 1000, 32)\n","classifier.fit(x_T, lrY)\n","\n","def error(pred, y):\n","    N = len(y)\n","    error = 0\n","    for i in range(N):\n","        if(pred[i] != y[i]):\n","          error += 1\n","    error /= N\n","    return error\n","\n","#Computando o erro dentro da amostra (Ein)\n","pred = classifier.predict(x_T)\n","print(\"Ein = \" + str(error(pred, lrY)))\n","\n","#Computando o erro fora da amostra (Eout)\n","pred = classifier.predict(x_test)\n","print(\"Eout = \" + str(error(pred, lr_y_test)))\n","\n","#Métricas de aprendizado\n","pred = classifier.predict(x_test)\n","print(classification_report(lr_y_test, pred))"]},{"cell_type":"markdown","metadata":{"id":"7eMziL89QAyK"},"source":["Atividade 6:\n","\n","1. Implemente a heurística *WeightDecay* utilizando a penalidade $L_2$"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"background_save":true},"id":"uZXxRdSyQBLX"},"outputs":[],"source":["from logisticregression import LogisticRegression\n","from sklearn.model_selection import train_test_split\n","import numpy as np\n","\n","import warnings\n","warnings.filterwarnings(\"error\")\n","\n","class WeightDecay:\n","  # Infere o vetor w da funçao hipotese\n","  #Executa a heuristica Weigth Decay\n","  def __init__(self, alpha=0.1, lamda=0.1):\n","    self.alpha = alpha # taxa de aprendizado\n","    self.lamda = lamda # fator de penalidade L2\n","\n","  def fit(self, X, y, max_iter=1000):\n","    self.w = np.zeros(X.shape[1]) # inicializa os pesos com zeros\n","    for _ in range(max_iter):\n","      h = self.predict_prob(X) # obtém as probabilidades preditas\n","      gradient = X.T @ (h - y) # calcula o gradiente\n","      penalty = self.lamda * self.w # aplica a penalidade L2\n","      gradient += penalty # adiciona o gradiente da penalidade L2\n","      self.w -= self.alpha * gradient # atualiza os pesos\n","        \n","    #funcao hipotese inferida pela regressa logistica  \n","  def predict_prob(self, X):\n","    return [(1 / (1 + np.exp(-(self.w[0] + self.w[1:].T @ x)))) for x in X]\n","\n","    #Predicao por classificação linear\n","  def predict(self, X):\n","    return [1 if (1 / (1 + np.exp(-(self.w[0] + self.w[1:].T @ x)))) \u003e= 0.5 \n","          else -1 for x in X]\n","\n","  def getW(self):\n","    return self.w\n","\n","  def getRegressionY(self, regressionX, shift=0):\n","    return (-self.w[0]+shift - self.w[1]*regressionX) / self.w[2]\n","    \n","  def getEout(self, pred, y_val):\n","    #Computando o erro quadrático (Eout)\n","    N_val = len(y_val)\n","    eOut = 0\n","    for i in range(N_val):\n","      if(pred[i] != y_val[i]):\n","        eOut += 1 \n","      eOut /= N_val\n","      return eOut"]},{"cell_type":"markdown","metadata":{"id":"Fyr1aWvJQGbI"},"source":["**Atividade 7:**\n","\n","1. Utilize a classe WeightDecay para inferir aprendizado dos dados de treinamento;\n","2. Compute o $E_{in}$ sobre os dados de treino e o $E_{out}$ sobre os dados de teste;\n","3. Compute as métricas de aprendizado sobre os dados de teste e compare os resutlados."]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"background_save":true},"id":"SGqo7qhUQHrY"},"outputs":[],"source":["from sklearn.metrics import classification_report\n","\n","#Weigth Decay heuristic\n","classifier = WeightDecay()\n","classifier.fit(x_T, lrY)\n","\n","#Computando o erro dentro da amostra (Ein)\n","pred = classifier.predict(x_T)\n","print(\"\\nEin = \" + str(error(pred, lrY)))\n","\n","#Computando o erro dentro da amostra (Eout)\n","pred = classifier.predict(x_test)\n","print(\"Eout = \" + str(error(pred, lr_y_test)))\n","\n","#Métricas de aprendizado\n","pred = classifier.predict(x_test)\n","print(classification_report(lr_y_test, pred))"]},{"cell_type":"markdown","metadata":{"id":"VI3GaycEKXh-"},"source":["Atividade 8:\n","\n","1. Implementar a validação da Logistic Regression através da classe *GridSearchCV* do pacote *sklearn.model_selection*;\n","2. Imprimir os parâmetros do melhor classificador;\n","3. Imprimir os $E_{in}$ e $E_{out}$  e as métricas de aprendizado.\n","\n","Parâmetros:\n","\n","\n","*   *estimator* : instância do classificador cujos hiperparâmetros serão analisados;\n","*   *cv* : número de divisões do conjunto de treinamento para ser usado na técnica de validação cruzada (10 é um bom valor observado na prática);\n","*   *param_grid* : conjunto de parâmetros a serem combinados durante a fase de validação.\n"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"background_save":true},"id":"elugBPN4DAWo"},"outputs":[],"source":["from sklearn.linear_model import LogisticRegression\n","from sklearn.model_selection import GridSearchCV\n","from sklearn.metrics import classification_report, accuracy_score\n","import numpy as np\n","\n","lr = LogisticRegression(solver='newton-cg')\n","\n","# Create the random grid\n","param_grid = {\n","              'C' : np.logspace(-5, 5, 10),\n","              }\n","\n","CV_rf = GridSearchCV(estimator=lr, param_grid=param_grid, cv = 10, verbose=2, n_jobs=-1)\n","\n","# Fit the random search model\n","CV_rf.fit(x_T, y_T)\n","\n","print(CV_rf.best_estimator_)\n","\n","\n","print('Ein: %0.4f' % (1 - accuracy_score(y_T, CV_rf.predict(x_T))))\n","print('Eout: %0.4f' % (1 - accuracy_score(y_test, CV_rf.predict(x_test))))\n","print(classification_report(y_test, CV_rf.predict(x_test)))"]}],"metadata":{"colab":{"name":"","version":""},"kernelspec":{"display_name":"Python 3","name":"python3"}},"nbformat":4,"nbformat_minor":0}